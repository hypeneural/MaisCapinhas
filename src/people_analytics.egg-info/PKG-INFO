Metadata-Version: 2.4
Name: people-analytics
Version: 0.1.0
Summary: Multi-store people analytics pipeline with DB job queue
License: Proprietary
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: pydantic>=2.7
Requires-Dist: pydantic-settings>=2.2
Requires-Dist: PyYAML>=6.0
Requires-Dist: SQLAlchemy>=2.0
Requires-Dist: alembic>=1.13
Requires-Dist: typer>=0.12
Requires-Dist: rich>=13.7
Requires-Dist: python-dotenv>=1.0
Requires-Dist: fastapi>=0.110
Requires-Dist: uvicorn>=0.27
Provides-Extra: vision
Requires-Dist: opencv-python>=4.9; extra == "vision"
Requires-Dist: ultralytics>=8.2; extra == "vision"
Requires-Dist: supervision>=0.20; extra == "vision"
Provides-Extra: dev
Requires-Dist: pytest>=7.4; extra == "dev"

# People Analytics

Modular offline pipeline to ingest security videos by store/camera/date, queue jobs in the database, and compute KPIs. The design favors a simple DB-backed job queue and a stage-based vision pipeline so new KPIs can be added without reworking the core flow.

## Video folder structure (required)

Do not depend on filename. Depend on folder structure.

```
/var/people_analytics/videos/
  store=001/
    camera=entrance/
      date=2025-12-31/
        14-00-00__14-10-00.mp4
        14-10-00__14-20-00.mp4
```

## Project layout

```
apps/
  cli.py                 CLI (ingest, process, kpi-rebuild, staff-rebuild)
  api/                   FastAPI routes (health, stores, segments, kpis)
  worker/                DB queue worker

src/people_analytics/
  core/                  settings, config, time utils
  storage/               path parser, scanner, fingerprints
  db/                    models, crud, session
  vision/                pipeline + stages (detect, track, count, staff)
  kpi/                   aggregators and rebuild

config/                  stores, cameras, shifts
var/                     logs, cache, debug_frames
scripts/                 systemd service files
tests/                   unit tests
```

## Flow

1) `ingest` scans the video root and inserts `video_segments`.
2) Each new segment becomes a `PROCESS_SEGMENT` job.
3) The worker processes segments, writes `people_flow_events`, then queues `KPI_REBUILD`.
4) `kpi_rebuild` aggregates hourly and shift KPIs.

## Quickstart (local)

1) Create and activate a venv
2) Install dependencies
3) Copy `.env.example` to `.env` and adjust paths
4) Create the local schema
5) Ingest videos
6) Run worker
7) Start API

Example:

```
python -m venv .venv
.\.venv\Scripts\activate
python -m pip install -e .
copy .env.example .env
python -m apps.cli init-db
python -m apps.cli ingest
python -m apps.worker.worker
uvicorn apps.api.main:app --reload --host 0.0.0.0 --port 8000
```

## Commands

```
python -m apps.cli ingest
python -m apps.cli process --path <video_file>
python -m apps.cli kpi-rebuild --date 2025-12-31 --store-id 1 --camera-id 1
```

## Notes

- Postgres is recommended for `SELECT ... FOR UPDATE SKIP LOCKED` when running multiple workers.
- SQLite is fine for single-worker dev and local schema creation.
- The worker requeues stale jobs based on `JOB_LOCK_TIMEOUT`.
- `python -m apps.cli process --path <file>` prints JSON output after reading a video.

## Output JSON example

```
{
  "segment": {
    "store_code": "001",
    "camera_code": "entrance",
    "start_time": "2025-12-31T14:00:00-03:00",
    "end_time": "2025-12-31T14:10:00-03:00"
  },
  "counts": {
    "in": 0,
    "out": 0,
    "staff_in": 0,
    "staff_out": 0
  },
  "events": [],
  "presence_samples": [],
  "meta": {
    "frames_read": 0,
    "duration_s": null,
    "errors": ["opencv-not-installed"]
  }
}
```
